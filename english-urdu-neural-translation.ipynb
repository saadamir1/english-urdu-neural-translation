{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assg 02\n",
    "# Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# English to Urdu Translation using Transformers\n",
    "# Implementation based on the paper \"Attention is All You Need\" (Vaswani et al.)\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "from sacrebleu import corpus_bleu\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Data Loading and Preprocessing\n",
    "class ParallelCorpusDataset(Dataset):\n",
    "    def __init__(self, english_texts, urdu_texts, tokenizer, max_length=128):\n",
    "        self.english_texts = english_texts\n",
    "        self.urdu_texts = urdu_texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.english_texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        english_text = str(self.english_texts[idx])\n",
    "        urdu_text = str(self.urdu_texts[idx])\n",
    "        \n",
    "        # Tokenize inputs\n",
    "        inputs = self.tokenizer(english_text, return_tensors=\"pt\", max_length=self.max_length, \n",
    "                               padding=\"max_length\", truncation=True)\n",
    "        \n",
    "        # Tokenize targets\n",
    "        with self.tokenizer.as_target_tokenizer():\n",
    "            targets = self.tokenizer(urdu_text, return_tensors=\"pt\", max_length=self.max_length, \n",
    "                                    padding=\"max_length\", truncation=True)\n",
    "        \n",
    "        input_ids = inputs.input_ids.squeeze()\n",
    "        attention_mask = inputs.attention_mask.squeeze()\n",
    "        labels = targets.input_ids.squeeze()\n",
    "        \n",
    "        # Replace padding token id with -100 for CrossEntropyLoss to ignore\n",
    "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "        \n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels\n",
    "        }\n",
    "\n",
    "def load_data(filepath):\n",
    "    \"\"\"Load data from CSV file with English and Urdu parallel sentences\"\"\"\n",
    "    df = pd.read_csv(filepath)\n",
    "    return df['english'].tolist(), df['urdu'].tolist()\n",
    "\n",
    "def download_kaggle_dataset():    \n",
    "    os.makedirs(\"data\", exist_ok=True)\n",
    "    \n",
    "    sample_data = {\n",
    "        'english': [\n",
    "            'Hello, how are you?',\n",
    "            'My name is Saad.',\n",
    "            'I live in Islamabad.',\n",
    "            'The weather is nice today.',\n",
    "            'I love programming.',\n",
    "        ],\n",
    "        'urdu': [\n",
    "            'ہیلو، آپ کیسے ہیں؟',\n",
    "            'میرا نام سعد ہے۔',\n",
    "            'میں اسلام آباد میں رہتا ہوں۔',\n",
    "            'آج موسم اچھا ہے۔',\n",
    "            'مجھے پروگرامنگ پسند ہے۔',\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    sample_df = pd.DataFrame(sample_data)\n",
    "    sample_df.to_csv('data/sample_en_ur_corpus.csv', index=False)\n",
    "    print(\"Created a sample dataset at data/sample_en_ur_corpus.csv\")\n",
    "\n",
    "\n",
    "# Training and Evaluation Functions\n",
    "def train_model(model, train_dataloader, optimizer, device, num_epochs=5):\n",
    "    model.train()\n",
    "    training_losses = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        for batch in progress_bar:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            # Apply gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            progress_bar.set_postfix({'loss': loss.item()})\n",
    "        \n",
    "        avg_loss = total_loss / len(train_dataloader)\n",
    "        training_losses.append(avg_loss)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Average Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    return training_losses\n",
    "\n",
    "def evaluate_model(model, test_dataloader, tokenizer, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    references = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_dataloader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            # Generate translations\n",
    "            generated_ids = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_length=128,\n",
    "                num_beams=4,\n",
    "                early_stopping=True\n",
    "            )\n",
    "            \n",
    "            # Decode the generated translations and actual targets\n",
    "            pred_texts = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "            \n",
    "            # Replace -100 with pad token ID for decoding\n",
    "            labels_for_decode = labels.clone()\n",
    "            labels_for_decode[labels_for_decode == -100] = tokenizer.pad_token_id\n",
    "            ref_texts = tokenizer.batch_decode(labels_for_decode, skip_special_tokens=True)\n",
    "            \n",
    "            predictions.extend(pred_texts)\n",
    "            references.extend(ref_texts)\n",
    "    \n",
    "    # Calculate BLEU score\n",
    "    bleu_score = corpus_bleu(predictions, [references]).score\n",
    "    \n",
    "    return bleu_score, predictions, references\n",
    "\n",
    "def translate_examples(model, tokenizer, english_texts, device):\n",
    "    model.eval()\n",
    "    translations = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for text in english_texts:\n",
    "            # Tokenize input text\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", max_length=128, padding=\"max_length\", truncation=True)\n",
    "            input_ids = inputs.input_ids.to(device)\n",
    "            attention_mask = inputs.attention_mask.to(device)\n",
    "            \n",
    "            # Generate translation\n",
    "            generated_ids = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_length=128,\n",
    "                num_beams=4,\n",
    "                early_stopping=True\n",
    "            )\n",
    "            \n",
    "            # Decode the generated translation\n",
    "            translation = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "            translations.append(translation)\n",
    "    \n",
    "    return translations\n",
    "\n",
    "# Main execution function\n",
    "def main():\n",
    "    # Step 1: Download/prepare dataset (in practice, download from Kaggle)\n",
    "    if not os.path.exists('data/sample_en_ur_corpus.csv'):\n",
    "        download_kaggle_dataset()\n",
    "    \n",
    "    # Path to your dataset\n",
    "    data_path = 'data/sample_en_ur_corpus.csv'\n",
    "    \n",
    "    # Step 2: Load and preprocess data\n",
    "    try:\n",
    "        english_texts, urdu_texts = load_data(data_path)\n",
    "        print(f\"Loaded {len(english_texts)} sentence pairs\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        # Fall back to the sample data if loading fails\n",
    "        english_texts = [\n",
    "            'Hello, how are you?',\n",
    "            'My name is Saad.',\n",
    "            'I live in Islamabad.',\n",
    "            'The weather is nice today.',\n",
    "            'I love programming.',\n",
    "        ]\n",
    "        urdu_texts = [\n",
    "            'ہیلو، آپ کیسے ہیں؟',\n",
    "            'مِرا نام سعد ہے۔',\n",
    "            'میں نیویارک میں رہتا ہوں۔',\n",
    "            'آج موسم اچھا ہے۔',\n",
    "            'مجھے پروگرامنگ پسند ہے۔',\n",
    "        ]\n",
    "        print(\"Using sample data instead\")\n",
    "    \n",
    "    # Step 3: Split the data into training and test sets\n",
    "    train_en, test_en, train_ur, test_ur = train_test_split(\n",
    "        english_texts, urdu_texts, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Step 4: Load pre-trained model and tokenizer\n",
    "    # Using Helsinki-NLP/opus-mt-en-ur from Hugging Face's Transformers\n",
    "    model_name = \"Helsinki-NLP/opus-mt-en-ur\"\n",
    "    tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "    model = MarianMTModel.from_pretrained(model_name).to(device)\n",
    "    \n",
    "    # Step 5: Create datasets and dataloaders\n",
    "    train_dataset = ParallelCorpusDataset(train_en, train_ur, tokenizer)\n",
    "    test_dataset = ParallelCorpusDataset(test_en, test_ur, tokenizer)\n",
    "    \n",
    "    batch_size = 16\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    \n",
    "    # Step 6: Initialize optimizer with learning rate scheduling\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
    "    \n",
    "    # Step 7: Train the model\n",
    "    print(\"Starting training...\")\n",
    "    num_epochs = 3  # Reduced for demonstration, can be increased for better results\n",
    "    training_losses = train_model(model, train_dataloader, optimizer, device, num_epochs=num_epochs)\n",
    "    \n",
    "    # Step 8: Evaluate the model\n",
    "    print(\"Evaluating model...\")\n",
    "    bleu_score, predictions, references = evaluate_model(model, test_dataloader, tokenizer, device)\n",
    "    print(f\"BLEU score: {bleu_score:.2f}\")\n",
    "    \n",
    "    # Step 9: Plot training loss\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, num_epochs + 1), training_losses, marker='o')\n",
    "    plt.title('Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid(True)\n",
    "    plt.savefig('training_loss.png')\n",
    "    \n",
    "    # Step 10: Demonstrate some translations\n",
    "    sample_texts = test_en[:5]\n",
    "    translations = translate_examples(model, tokenizer, sample_texts, device)\n",
    "    \n",
    "    print(\"\\nSample Translations:\")\n",
    "    for i, (source, target, pred) in enumerate(zip(sample_texts, test_ur[:5], translations)):\n",
    "        print(f\"\\nExample {i+1}:\")\n",
    "        print(f\"Source (English): {source}\")\n",
    "        print(f\"Target (Urdu): {target}\")\n",
    "        print(f\"Predicted (Urdu): {pred}\")\n",
    "    \n",
    "    # Step 11: Save the model\n",
    "    model_save_path = 'en_ur_transformer_model'\n",
    "    model.save_pretrained(model_save_path)\n",
    "    tokenizer.save_pretrained(model_save_path)\n",
    "    print(f\"\\nModel saved to {model_save_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
